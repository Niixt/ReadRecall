{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c454dcbc",
   "metadata": {},
   "source": [
    "# Draft notebook for the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bb043",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275c2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b959801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a6110",
   "metadata": {},
   "source": [
    "## Test book content loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd821b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loaders.book_content_loader as bcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8325f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"Martin Eden\"\n",
    "res_search = bcl.search_books(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f84b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_archive = bcl.get_book_archive_page(res_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68337543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book found with length: 930190 characters\n",
      " usually  a  pot  of  them,  cooked  and  ready \n",
      "at  hand,  for  they  took  the  place  of  butter  on  his  bread. \n",
      "Occasionally  he  graced  his  table  with  a  piece  of  round- \n",
      "steak,  or  with\n"
     ]
    }
   ],
   "source": [
    "book_text = bcl.fetch_book_text(url_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41060f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "LIGATURES = {\n",
    "    \"ﬀ\": \"ff\", \"ﬁ\": \"fi\", \"ﬂ\": \"fl\", \"ﬃ\": \"ffi\", \"ﬄ\": \"ffl\", \"ﬅ\": \"ft\", \"ﬆ\": \"st\"\n",
    "}\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    for lig, repl in LIGATURES.items():\n",
    "        text = text.replace(lig, repl)\n",
    "    # normalize different dashes and quotes\n",
    "    text = text.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \" - \").replace(\"\\u00AC\", \"-\")\n",
    "    text = text.replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
    "    # remove weird non-breaking spaces\n",
    "    text = text.replace(\"\\u00A0\", \" \")\n",
    "    return text\n",
    "\n",
    "def remove_uc_only_lines(text: str) -> str:\n",
    "    # heuristics: many header/footer lines are ALL CAPS short words (publisher, title)\n",
    "    lines = text.splitlines()\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        ln_stripped = ln.strip()\n",
    "        if not ln_stripped:\n",
    "            out.append(ln)\n",
    "            continue\n",
    "        # if line is short and mostly uppercase and not a sentence, drop it\n",
    "        if len(ln_stripped) < 60:\n",
    "            alpha_chars = re.sub(r'[^A-Za-z]', '', ln_stripped)\n",
    "            if alpha_chars and alpha_chars.upper() == alpha_chars and len(alpha_chars) > 3:\n",
    "                # avoid deleting lines that look like sentences (end with . ? !)\n",
    "                if not re.search(r'[.?!]\\s*$', ln_stripped) and 'chapter' not in ln_stripped.lower():\n",
    "                    # skip likely header/footer\n",
    "                    continue\n",
    "        out.append(ln)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def remove_page_numbers(text: str) -> str:\n",
    "    # Remove lines that are only numbers or numbers with small decorations\n",
    "    lines = text.splitlines()\n",
    "    newlines = []\n",
    "    for ln in lines:\n",
    "        if re.fullmatch(r'\\s*\\d+\\s*', ln):\n",
    "            continue\n",
    "        # also remove lines like \"Page 12\" (common)\n",
    "        if re.fullmatch(r'\\s*(page|pg|p\\.)\\s*\\d+\\s*', ln, flags=re.IGNORECASE):\n",
    "            continue\n",
    "        newlines.append(ln)\n",
    "    return \"\\n\".join(newlines)\n",
    "\n",
    "def fix_hyphenation(text: str) -> str:\n",
    "    # merge words split with hyphen at EOL:\n",
    "    # pattern: 'hy- \\nphenated' or 'hy-\\nphenated' -> 'hyphenated'\n",
    "    text = re.sub(r'([A-Za-z])-\\n([A-Za-z])', r'\\1\\2', text)\n",
    "    # also handle hyphen + spaces + newline\n",
    "    text = re.sub(r'([A-Za-z])-\\s*\\n\\s*([A-Za-z])', r'\\1\\2', text)\n",
    "    return text\n",
    "\n",
    "def reflow_paragraphs(text: str) -> str:\n",
    "    # Reflow lines within paragraphs: paragraphs separated by empty lines.\n",
    "    parts = re.split(r'\\n{2,}', text)\n",
    "    reflowed = []\n",
    "    for p in parts:\n",
    "        # strip leading/trailing spaces per paragraph\n",
    "        lines = [ln.strip() for ln in p.splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            reflowed.append(\"\")\n",
    "            continue\n",
    "        # join with single space\n",
    "        joined = \" \".join(lines)\n",
    "        # collapse multiple spaces\n",
    "        joined = re.sub(r'\\s+', ' ', joined).strip()\n",
    "        reflowed.append(joined)\n",
    "    return \"\\n\".join(reflowed)\n",
    "\n",
    "def dedupe_repeated_header_footer(text: str, page_break_token: str = None) -> str:\n",
    "    # If you have a page break token (like '\\f') use it. Otherwise guess by rough pages.\n",
    "    if page_break_token and page_break_token in text:\n",
    "        pages = text.split(page_break_token)\n",
    "    else:\n",
    "        # attempt naive page split if the source used form feed markers, else split by approx page length\n",
    "        approx_chars = 3000\n",
    "        pages = [text[i:i+approx_chars] for i in range(0, len(text), approx_chars)]\n",
    "    header_cands = Counter()\n",
    "    footer_cands = Counter()\n",
    "    first_lines = []\n",
    "    last_lines = []\n",
    "    for p in pages:\n",
    "        lines = [ln.strip() for ln in p.splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "        first_lines.append(lines[0][:120])\n",
    "        last_lines.append(lines[-1][:120])\n",
    "    # find frequent first/last lines\n",
    "    for ln in first_lines:\n",
    "        header_cands[ln] += 1\n",
    "    for ln in last_lines:\n",
    "        footer_cands[ln] += 1\n",
    "    # choose candidates that appear on many pages (threshold)\n",
    "    n_pages = max(1, len(pages))\n",
    "    headers = {ln for ln, c in header_cands.items() if c > max(1, n_pages*0.4)}\n",
    "    footers = {ln for ln, c in footer_cands.items() if c > max(1, n_pages*0.4)}\n",
    "    # remove these exact lines from the text\n",
    "    if headers or footers:\n",
    "        def drop_headers_footers_line(ln):\n",
    "            s = ln.strip()\n",
    "            if s in headers or s in footers:\n",
    "                return False\n",
    "            return True\n",
    "        out_lines = [ln for ln in text.splitlines() if drop_headers_footers_line(ln)]\n",
    "        return \"\\n\".join(out_lines)\n",
    "    return text\n",
    "\n",
    "def collapse_whitespace(text: str) -> str:\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # normalize repeated blank lines to two newlines for paragraph separation\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_book_text(raw: str, page_break_token: str = None) -> str:\n",
    "    t = raw\n",
    "    t = normalize_unicode(t)\n",
    "    t = dedupe_repeated_header_footer(t, page_break_token=page_break_token)\n",
    "    t = remove_page_numbers(t)\n",
    "    t = remove_uc_only_lines(t)\n",
    "    t = fix_hyphenation(t)\n",
    "    t = reflow_paragraphs(t)\n",
    "    t = collapse_whitespace(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab54dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930190\n"
     ]
    }
   ],
   "source": [
    "print(len(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e72f32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw = open('documents/Martin_Eden_raw.txt', 'r', encoding='utf-8').read()\n",
    "cleaned = clean_book_text(book_text, page_break_token='\\f')  # pass '\\f' if present\n",
    "open('documents/Martin_Eden_clean.txt', 'w', encoding='utf-8').write(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed17ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book text written to documents/Martin_Eden.txt\n"
     ]
    }
   ],
   "source": [
    "path_book_content = bcl.write_book_to_file(book_text, search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f3a19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'documents/Martin_Eden.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_book_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554c1c2",
   "metadata": {},
   "source": [
    "## RAG with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f704ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrieval.rag_retriever as rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cadab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1122 chunks from 49 chapters.\n",
      "Creating embeddings and vector store...\n",
      "Vector store created with 1122 chunks.\n",
      "Loading model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Attempting to load model with 4-bit quantization...\n",
      "Tokenizer loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b5430768ca4a64aede5c552d74c07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with 4-bit quantization!\n",
      "RAG system initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "rag = rr.LocalRAGSystem(documents_path=\"documents/Martin_Eden_clean.txt\",\n",
    "                        path_token_hf=\"..\\\\HF_TOKEN\",\n",
    "                        model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1506b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that Ruth is a woman who is the subject of Martin Eden's thoughts and desires. He has not yet met her in person, but his imagination is filled with visions of her, and he finds himself drawn to her.\n"
     ]
    }
   ],
   "source": [
    "result = rag.query(\"Who is Ruth?\", chapter_max=5)\n",
    "# result = rag.query(\"What is Ruth's full name?\")\n",
    "# result = rag.query(\"What is Martin final destination and what happened to him?\", chapter_max=50)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx = rag.docs[0].metadata['chapter_index']\n",
    "nb_char = 0\n",
    "for doc in rag.docs:\n",
    "    if doc.metadata['chapter_index'] != last_idx:\n",
    "        print(f\"Total char for chapter {last_idx}: {nb_char}\")\n",
    "        nb_char = 0\n",
    "        last_idx = doc.metadata['chapter_index']\n",
    "    # print(f\"Chapter_i {doc.metadata['chapter_index']}: {len(doc.page_content)}\")\n",
    "    nb_char += len(doc.page_content)\n",
    "# Print the last chapter's total\n",
    "print(f\"Total char for chapter {last_idx}: {nb_char}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readrecall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
